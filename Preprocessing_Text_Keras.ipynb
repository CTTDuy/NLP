{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['he', 'is', 'a', 'teacher'], ['she', 'is', 'a', 'teacher'], ['she', 'and', 'he', 'are', 'teachers'], ['he', 'is', 'a', 'student', 'and', 'she', 'is', 'a', 'student']]\n"
     ]
    }
   ],
   "source": [
    "texts = ['he is a teacher', 'she is a teacher', 'she and he are teachers', 'he is a student and she is a student']\n",
    "sentences = []\n",
    "for sent in texts:\n",
    "  sentences.append(nltk.word_tokenize(sent))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 1, 'a': 2, 'he': 3, 'she': 4, 'teacher': 5, 'and': 6, 'student': 7, 'are': 8, 'teachers': 9}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 1. 0. 1. 1.]\n",
      " [0. 2. 2. 1. 1. 0. 1. 2. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#he is a teacher\n",
    "#she is a teacher\n",
    "#she and he are teachers\n",
    "#he is a student and she is a student\n",
    "x = tokenizer.texts_to_matrix(sentences, mode='count')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 5)\n",
      "Index(['v1', 'v2', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], dtype='object')\n",
      "     v1                                                 v2 Unnamed: 2  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
      "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
      "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
      "5  spam  FreeMsg Hey there darling it's been 3 week's n...        NaN   \n",
      "6   ham  Even my brother is not like to speak with me. ...        NaN   \n",
      "7   ham  As per your request 'Melle Melle (Oru Minnamin...        NaN   \n",
      "8  spam  WINNER!! As a valued network customer you have...        NaN   \n",
      "9  spam  Had your mobile 11 months or more? U R entitle...        NaN   \n",
      "\n",
      "  Unnamed: 3 Unnamed: 4  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n",
      "5        NaN        NaN  \n",
      "6        NaN        NaN  \n",
      "7        NaN        NaN  \n",
      "8        NaN        NaN  \n",
      "9        NaN        NaN  \n"
     ]
    }
   ],
   "source": [
    "filename = 'spam.csv'\n",
    "df = pd.read_csv(filename, encoding= 'latin-1')\n",
    "#df = pd.read_csv(filename, delimiter='\\t', header=None, skipinitialspace=True, encoding= 'latin-1')\n",
    "#df = pd.read_csv(filename, encoding= 'utf-8')\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'\n",
      " 'Ok lar... Joking wif u oni...'\n",
      " \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"\n",
      " 'U dun say so early hor... U c already then say...'\n",
      " \"Nah I don't think he goes to usf, he lives around here though\"]\n",
      "['ham' 'ham' 'spam' 'ham' 'ham' 'spam' 'ham' 'ham' 'spam' 'spam' 'ham'\n",
      " 'spam' 'spam' 'ham' 'ham' 'spam' 'ham' 'ham' 'ham' 'spam']\n"
     ]
    }
   ],
   "source": [
    "data =df.values\n",
    "#print(data[:2])\n",
    "lines = data[:,1]\n",
    "labels = data[:,0]\n",
    "print(lines[:5])\n",
    "print(labels[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'Available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'Cine', 'there', 'got', 'amore', 'wat', '...']\n"
     ]
    }
   ],
   "source": [
    "# tách từ trong từng câu\n",
    "sentences = [nltk.word_tokenize(sent) for sent in lines]\n",
    "#2-gram\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 1. 1. 0.]\n",
      " [0. 0. 1. ... 0. 0. 1.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "#matrix\n",
    "X = tokenizer.texts_to_matrix(sentences, mode='binary')\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# thay logistic-regression -> naive bayes, svm\n",
    "model_LR = LogisticRegression()\n",
    "model_LR = model_LR.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99       949\n",
      "        spam       0.99      0.89      0.93       166\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.98      0.94      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = model_LR.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Cao', 'Tran', 'Tuan', 'Duy']]\n"
     ]
    }
   ],
   "source": [
    "input_sent = 'Cao Tran Tuan Duy'\n",
    "\n",
    "# Step 1: tokenize\n",
    "input_sentences = []\n",
    "input_sentences.append(nltk.word_tokenize(input_sent))\n",
    "\n",
    "print(input_sentences)\n",
    "\n",
    "#input_sentences = [nltk.word_tokenize(input_sent)]\n",
    "#print(input_sentences)\n",
    "\n",
    "\n",
    "# Step 2: chuyển list of tokens -> vector of number\n",
    "\n",
    "# texts_to_matrix\n",
    "# tokenizer.texts_to_matrix(input_sentences, mode='binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X_input = tokenizer.texts_to_matrix(input_sentences, mode='binary')\n",
    "print(X_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham']\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_LR.predict(X_input)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
